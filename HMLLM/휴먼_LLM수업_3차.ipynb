{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN78YmPZgp6ONYyMWbgqw6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yul77/Review/blob/main/HMLLM/%ED%9C%B4%EB%A8%BC_LLM%EC%88%98%EC%97%85_3%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification"
      ],
      "metadata": {
        "id": "unQMdcOybggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 모델별 장단점\n",
        "\n",
        "### 1. **GPT-4o (OpenAI)**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **성능**: GPT-4 기반으로 높은 언어 처리 능력, 문맥 이해 및 질의 응답 성능이 우수함.\n",
        "- **범용성**: 다양한 작업에 강력한 성능을 발휘하여, 생성, 번역, 분석 등에 매우 유용.\n",
        "- **지속적인 학습**: 최신 데이터에 대한 접근 및 업그레이드된 성능을 제공.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **비용**: 높은 비용이 발생할 수 있으며, 특히 대규모 응용 프로그램에서 사용하기 부담이 큼.\n",
        "- **리소스 요구**: 높은 컴퓨팅 리소스가 필요하여 개인 개발자나 소규모 팀에서 사용하기 부담스러울 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **고급 자연어 처리**: 복잡한 질의응답 시스템, 번역, 텍스트 요약 및 생성.\n",
        "- **상담 서비스**: 사용자와 자연스럽게 대화하며 다양한 주제에 대한 심층적인 대화 및 정보 제공.\n",
        "- **창의적 작업**: 소설, 시 등 창의적인 콘텐츠 생성.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "자연어 처리(NLP), 고객 지원, 창의적 콘텐츠 생성, 복잡한 언어 기반 작업.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[GPT-4o 웹사이트](https://openai.com/gpt-4)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **GPT-4o-mini (OpenAI)**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **경량화**: GPT-4o의 축소된 버전으로, 더 적은 리소스로 작동 가능하여 경량 시스템에 적합.\n",
        "- **비용 절감**: GPT-4o보다 저렴하게 사용할 수 있음.\n",
        "- **범용성 유지**: 기본적인 언어 처리 능력을 유지하면서도 리소스 효율적임.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **성능 한계**: GPT-4o에 비해 복잡한 작업에서 성능이 다소 낮을 수 있음.\n",
        "- **대규모 응용 프로그램 제한**: 특정 대규모 작업이나 복잡한 질의 응답에서 한계를 보일 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **경량 챗봇**: 소규모 응용 프로그램에서 경량형 챗봇을 빠르게 개발.\n",
        "- **실시간 고객 지원**: 중간 복잡도의 질문에 답변하는 실시간 채팅 서비스.\n",
        "- **기본적인 언어 처리**: 텍스트 분류, 간단한 자연어 이해 작업.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "챗봇, 기본적인 자연어 처리, 중소기업 고객 지원 시스템.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[GPT-4o-mini 웹사이트](https://openai.com/gpt-4)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Claude 3.5 Sonnet (Anthropic)**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **안전성 강화**: AI 윤리와 안전성을 강조하는 설계로, 민감한 주제에 대해 더 신중한 답변을 제공.\n",
        "- **사용자 친화적**: 사용자 피드백에 기반하여 직관적이고 이해 가능한 응답을 제공.\n",
        "- **비용 효율성**: 상대적으로 저렴한 사용 비용으로 고성능을 제공.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **성능 제한**: 복잡한 작업에서 OpenAI의 GPT 모델만큼 정교하지 않으며, 특정 언어 및 문맥 처리에 약점을 가질 수 있음.\n",
        "- **데이터 최신성**: 최신 데이터에 대한 접근이 제한될 수 있어 특정 최신 정보에 대한 응답이 부족할 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **안전한 대화형 AI**: 윤리적 고려가 중요한 환경에서 민감한 질문에 대한 안전한 응답 제공.\n",
        "- **기업용 AI 비서**: 내부 데이터 처리 및 민감한 데이터 처리에서 안전한 답변 제공.\n",
        "- **교육용 챗봇**: 교육용 콘텐츠 생성 및 교사의 보조 역할.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "안전한 대화형 AI, 기업 내부 솔루션, 교육.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[Claude 3.5 웹사이트](https://www.anthropic.com/)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Perplexity (Perplexity AI)**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **최신 정보 기반**: 인터넷 검색을 통해 실시간 데이터를 기반으로 답변을 제공하여 최신 정보에 매우 강함.\n",
        "- **설명 가능성**: 응답의 출처를 명시하는 기능이 있어 정보의 신뢰성을 높임.\n",
        "- **사용자 피드백 반영**: 사용자 피드백을 빠르게 반영하여 응답 품질을 개선.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **모델 성능**: GPT-4 등의 상위 모델에 비해 언어 처리 능력은 떨어질 수 있음.\n",
        "- **제한된 창의성**: 고정된 형식의 질의 응답에서는 강하지만, 창의적인 콘텐츠 생성에는 다소 한계가 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **실시간 정보 검색**: 최신 정보를 바탕으로 실시간으로 답변을 제공하는 지식 검색 엔진.\n",
        "- **뉴스 모니터링**: 최신 뉴스나 트렌드에 대한 정보를 실시간으로 검색하여 요약.\n",
        "- **빠른 질의응답 시스템**: 실시간 데이터 기반의 질문 응답 시스템 구축.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "실시간 정보 검색, 트렌드 분석, 뉴스 모니터링.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[Perplexity 웹사이트](https://www.perplexity.ai/)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **LLaMA 3.1 (Meta)**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **개방성**: 공개된 모델로, 연구자나 개발자들이 쉽게 접근할 수 있어 확장성과 유연성이 뛰어남.\n",
        "- **성능 대비 비용 효율성**: 높은 성능을 제공하면서도 상업적 모델에 비해 저렴한 비용으로 사용할 수 있음.\n",
        "- **경량화**: 다양한 크기의 모델을 제공하여 리소스에 맞춰 사용할 수 있음.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **상용화 부족**: 상용 제품에서 OpenAI나 Google의 모델만큼 폭넓게 사용되지 않음.\n",
        "- **최적화 필요**: 모델의 성능을 최대한으로 발휘하려면 추가적인 최적화 작업이 필요할 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **연구 및 개발**: 연구 및 비영리 조직에서 자유롭게 사용할 수 있는 모델.\n",
        "- **텍스트 분석**: 대량의 텍스트 데이터를 분석하여 인사이트를 도출하는 응용 프로그램.\n",
        "- **비용 절감이 중요한 프로젝트**: 대규모 AI 시스템을 저비용으로 운영하고자 하는 프로젝트.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "연구 및 개발, 텍스트 분석, AI 비용 절감 프로젝트.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[LLaMA 3.1 웹사이트](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Gemma2**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **특정 도메인 특화**: 특정 도메인이나 산업에 특화된 모델로, 고도로 세밀한 답변을 제공할 수 있음.\n",
        "- **빠른 처리 속도**: 경량화된 모델로, 빠른 처리 속도를 제공하여 실시간 응답이 필요할 때 유용함.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **범용성 부족**: 범용 작업에서는 다른 대형 모델들만큼의 성능을 발휘하지 못할 수 있음.\n",
        "- **한정된 사용 사례**: 특정 도메인에 최적화되어 있어 다양한 작업에 활용하는 데는 제한이 있을 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **전문 도메인 응답**: 특정 산업 또는 연구 분야에 특화된 질문에 빠르게 응답.\n",
        "- **실시간 데이터 처리**: 빠른 처리 속도가 요구되는 실시간 응답 시스템.\n",
        "- **분야별 전문 지식 제공**: 의료, 금융, 법률 등 특정 산업에 맞춘 지식 제공.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "전문 도메인 특화 시스템, 실시간 데이터 처리, 특정 산업 솔루션.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[Gemma2 웹사이트](https://gemma2.com/) (예시)\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Upstage**\n",
        "\n",
        "**장점:**\n",
        "\n",
        "- **사용자 정의 가능**: 사용자 맞춤형 AI 기능을 제공하여, 특정 비즈니스나 개인의 요구에 맞춘 모델을 개발 가능.\n",
        "- **높은 유연성**: 다양한 환경에서 쉽게 통합할 수 있는 유연한 아키텍처 제공.\n",
        "- **커뮤니티 지원**: 개발자 커뮤니티와 협력하여 모델을 지속적으로 개선.\n",
        "\n",
        "**단점:**\n",
        "\n",
        "- **성능 변동성**: 사용자가 모델을 조정해야 할 필요가 있어 성능이 상황에 따라 다를 수 있음.\n",
        "- **규모의 한계**: 대규모 상용 애플리케이션에서의 사용 사례가 적어\n",
        "\n",
        "상용화된 모델들과의 비교에서 한계가 있을 수 있음.\n",
        "\n",
        "**응용 사례**:\n",
        "\n",
        "- **비즈니스 특화 AI**: 기업 맞춤형 솔루션을 구축하여 비즈니스 요구에 맞춘 모델을 개발.\n",
        "- **맞춤형 챗봇**: 고객 지원, 내부 소통, 맞춤형 정보 제공을 위한 기업형 챗봇 개발.\n",
        "- **고객 맞춤형 서비스**: 특정 고객 요구에 맞춘 개인화된 서비스 제공.\n",
        "\n",
        "**적합한 분야**:\n",
        "\n",
        "기업 솔루션, 맞춤형 챗봇, 고객 지원 시스템.\n",
        "\n",
        "**웹사이트**:\n",
        "\n",
        "[Upstage 웹사이트](https://www.upstage.ai/)"
      ],
      "metadata": {
        "id": "CPXl4mF8bgZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text 분류 실습 1\n",
        "\n",
        "영화 리뷰 데이터를 사용해 긍정/부정 감성 분석 모델을 학습시킨 후, LLM을 활용해 리뷰의 감성을 예측하고 그 이유를 설명하는 텍스트를 생성합니다. Scikit-learn 라이브러리를 활용해 텍스트 분류 모델을 학습시키고, api key를 사용해 GPT-3.5 turbo를 사용해 분류 결과에 대한 설명을 생성합니다."
      ],
      "metadata": {
        "id": "Gnkb3VvUby6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install openai==0.28\n",
        "!pip install openai scikit-learn pandas"
      ],
      "metadata": {
        "id": "2qWhtciRb1TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "we8A-aapcKPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R185e0zBLdWM"
      },
      "outputs": [],
      "source": [
        "# 2. 라이브러리 임포트\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split                             # 훈련 세트와 테스트 세트로 분류. 데이터를 무작위로 섞어서 지정한 비율로 분할\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                      # 텍스트 데이터를 수치형 벡터로 변환. 특히 TF-IDF 사용하여 단어의 중요도 반영\n",
        "from sklearn.naive_bayes import MultinomialNB                                    # 다항분포를 따르는 나이브 베이즈 분류기. 단어의 출현 빈도를 기반으로 분류\n",
        "from sklearn.pipeline import Pipeline                                            # 데이터 전처리와 모델 학습과정을 하나의 파이프라인으로 연결.코드 가독성 향상.유지보수 용이\n",
        "from sklearn.metrics import accuracy_score                                       # 모델의 예측 정확도 평가지표. 전체 중 맞는 예측의 비율을 나타낸다.\n",
        "import openai\n",
        "\n",
        "# 3. 영화 리뷰 데이터 생성 및 데이터프레임 만들기\n",
        "data = {\n",
        "    \"review_id\": range(1, 21),\n",
        "    \"review\": [\n",
        "        \"이 영화는 정말 환상적이었어요! 스토리가 매우 흡입력이 있었습니다.\",\n",
        "        \"영화의 모든 순간이 싫었어요. 연기가 형편없었어요.\",\n",
        "        \"비주얼은 멋졌지만, 줄거리가 부족했어요.\",\n",
        "        \"즐거운 관람이었고, 멋진 배우들의 연기였습니다.\",\n",
        "        \"제 취향이 아니었어요, 너무 느리고 지루했어요.\",\n",
        "        \"정말 감동적인 이야기였고, 눈물이 났어요.\",\n",
        "        \"대사가 너무 뻔했어요. 예측 가능한 전개였어요.\",\n",
        "        \"음악이 정말 좋았고, 분위기를 잘 살렸어요.\",\n",
        "        \"기대 이하였습니다. 더 잘 만들 수 있었을 것 같아요.\",\n",
        "        \"배우들의 케미가 좋았고, 재미있게 봤어요.\",\n",
        "        \"액션 장면이 너무 많아서 지루했어요.\",\n",
        "        \"스토리가 탄탄하고 캐릭터가 매력적이었어요.\",\n",
        "        \"너무 어두운 내용이라서 기분이 안 좋았어요.\",\n",
        "        \"코미디 요소가 많아서 웃으면서 봤습니다.\",\n",
        "        \"전반적으로 지루하고 감정선이 약했어요.\",\n",
        "        \"화면이 아름답고, 연출이 훌륭했어요.\",\n",
        "        \"캐릭터들이 매력적이지 않아서 몰입이 안 됐어요.\",\n",
        "        \"끝까지 긴장감이 넘쳐서 손에 땀을 쥐게 했어요.\",\n",
        "        \"스토리가 엉성하고, 끝이 허무했어요.\",\n",
        "        \"다시 보고 싶을 만큼 좋은 영화였습니다.\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"부정\", \"혼합\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 4. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "                                                                                   # 10개중 2개는 빼고 8개는 학습시킨다.\n",
        "                                                                                   # random_state=42 : 일정하게 랜덤 시키기 위해 아무숫자나 준것.베타때문에 넣는것\n",
        "\n",
        "# 5. 한국어 불용어 목록 정의 (예시로 일부 불용어 추가) -> 영어는 자동으로 되는데 한글은 아쉬운놈이 학습시켜야됨.\n",
        "korean_stopwords = [\"정말\", \"매우\", \"모든\", \"너무\", \"정도\", \"이\", \"그\", \"저\", \"에서\", \"그리고\"]\n",
        "\n",
        "# 6. 감성 분석 모델 정의 및 학습(TD-IDF) 베이즈 알고리즘, 특징 파라미터는 tf-if 사용해본 것. 보통 bert 씀.\n",
        "model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=korean_stopwords)),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 7. 검증 데이터에 대한 모델 평가\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 8. OpenAI API 설정\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# 9. 리뷰에 대한 설명을 생성하는 함수 정의\n",
        "def generate_explanation(review, prediction):                                   # review : 분석할 영화 리뷰 텍스트 // prediction : 모델이 예측한 리뷰감서(긍/부정)\n",
        "    prompt = (                                                                  # prompt : 구체적인 설명을 요청하기 위한 텍스트.\n",
        "        f\"다음 영화 리뷰의 감성은 '{prediction}'입니다. 리뷰: \\\"{review}\\\". \"   # prediction : 예측 된 감성을 명시. // review : 실제 리뷰 텍스트 제공.\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(                                    # openai의 채팅모델 api를 호출하여 응답 생성.\n",
        "        model=\"gpt-3.5-turbo\",                                                  # model: 사용할 모델 지정. -> OpenAI의 GPT-3.5 Turbo모델을 사용\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],                         # message : 대화 형식의 입력. 사용자 역할의 메시지로 프롬프트 전달.\n",
        "        max_tokens=150                                                          # max_tokens : 응답의 최대 토큰 수 설정. >> 이거 다 돈임. 너무 크게 잡으면 거지됨.\n",
        "    )\n",
        "    return response.choices[0].message['content']                               # 응답처리 및 반환. 첫번째 선택지 (choice[0])의 메시지 내용을 추출하여 반환.\n",
        "\n",
        "# 10. 예측 및 설명 생성\n",
        "for review in X_test[:5]:  # 일부 테스트 데이터에 대해 예제 실행                 루프 시작. X_test[:5]: 테스트 데이터셋의 첫 5개 리뷰를 선택하여 반복문 실행.\n",
        "    prediction = model.predict([review])[0]                                      # 현재 리뷰의 감성을 예측.\n",
        "    explanation = generate_explanation(review, prediction)                       # 앞서 정의한 generate_explanation 함수를 호출하여 예측에 대한 설명을 생성.\n",
        "    print(f\"리뷰: {review}\\n예측: {prediction}\\n설명: {explanation}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 비용 및 속도 관리.\n",
        "- OpenAI의 API 호출은 사용량에 따라 비용이 발생할 수 있으므로 예산 관리를 위해 호출 횟수를 제항하거나, 비용을 모니터링 하는 것이 중요.\n",
        "\n",
        "- API호출은 네트워크 지연과 모델 응답 시간게 영향을 받을 수 있음."
      ],
      "metadata": {
        "id": "npry9JPOMkzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF로 학습\n",
        "\n",
        "\n",
        "# 2. 라이브러리 임포트\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import openai\n",
        "\n",
        "# 3. 영화 리뷰 데이터 생성 및 데이터프레임 만들기\n",
        "data = {\n",
        "    \"review_id\": range(1, 21),\n",
        "    \"review\": [\n",
        "        \"이 영화는 정말 환상적이었어요! 스토리가 매우 흡입력이 있었습니다.\",\n",
        "        \"영화의 모든 순간이 싫었어요. 연기가 형편없었어요.\",\n",
        "        \"비주얼은 멋졌지만, 줄거리가 부족했어요.\",\n",
        "        \"즐거운 관람이었고, 멋진 배우들의 연기였습니다.\",\n",
        "        \"제 취향이 아니었어요, 너무 느리고 지루했어요.\",\n",
        "        \"정말 감동적인 이야기였고, 눈물이 났어요.\",\n",
        "        \"대사가 너무 뻔했어요. 예측 가능한 전개였어요.\",\n",
        "        \"음악이 정말 좋았고, 분위기를 잘 살렸어요.\",\n",
        "        \"기대 이하였습니다. 더 잘 만들 수 있었을 것 같아요.\",\n",
        "        \"배우들의 케미가 좋았고, 재미있게 봤어요.\",\n",
        "        \"액션 장면이 너무 많아서 지루했어요.\",\n",
        "        \"스토리가 탄탄하고 캐릭터가 매력적이었어요.\",\n",
        "        \"너무 어두운 내용이라서 기분이 안 좋았어요.\",\n",
        "        \"코미디 요소가 많아서 웃으면서 봤습니다.\",\n",
        "        \"전반적으로 지루하고 감정선이 약했어요.\",\n",
        "        \"화면이 아름답고, 연출이 훌륭했어요.\",\n",
        "        \"캐릭터들이 매력적이지 않아서 몰입이 안 됐어요.\",\n",
        "        \"끝까지 긴장감이 넘쳐서 손에 땀을 쥐게 했어요.\",\n",
        "        \"스토리가 엉성하고, 끝이 허무했어요.\",\n",
        "        \"다시 보고 싶을 만큼 좋은 영화였습니다.\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"부정\", \"혼합\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data) # 데이터 수집\n",
        "\n",
        "# 4. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. 한국어 불용어 목록 정의 (예시로 일부 불용어 추가) = 데이터 전처리\n",
        "korean_stopwords = [\"정말\", \"매우\", \"모든\", \"너무\", \"정도\", \"이\", \"그\", \"저\", \"에서\", \"그리고\"]\n",
        "\n",
        "# 6. 감성 분석 모델 정의 및 학습(TD-IDF)\n",
        "model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=korean_stopwords)),\n",
        "    ('classifier', MultinomialNB()) # = 나이브 베이즈 알고리즘 사용\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train) # 학습\n",
        "\n",
        "# 7. 검증 데이터에 대한 모델 평가\n",
        "y_pred = model.predict(X_test) # 아까 빼둔 20%의 x값은 넣어서 정확도를 본 것\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # accuracy = 분류식 <=> 회귀식\n",
        "\n",
        "# 8. OpenAI API 설정\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# 9. 리뷰에 대한 설명을 생성하는 함수 정의\n",
        "def generate_explanation(review, prediction):\n",
        "    prompt = (\n",
        "        f\"다음 영화 리뷰의 감성은 '{prediction}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}], # 유저 변경도 가능.\n",
        "        max_tokens=150 # 사용료임. 크게 잡으면 거지됨\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# 10. 예측 및 설명 생성\n",
        "for review in X_test[:5]:  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    prediction = model.predict([review])[0]\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {prediction}\\n설명: {explanation}\\n\")"
      ],
      "metadata": {
        "id": "e_D-_Hkrb697"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec으로 학습, review를 20개, vector_size=100으로 할 때와 review를 100개, vector_size=200으로 했을 때의 Accuracy 비교"
      ],
      "metadata": {
        "id": "KTjnKDfTcBUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#응용\n",
        "\n",
        "# 2. 라이브러리 임포트\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import openai\n",
        "\n",
        "# 3. 영화 리뷰 데이터 생성 및 데이터프레임 만들기\n",
        "# 3. 영화 리뷰 데이터 생성 및 데이터프레임 만들기\n",
        "data = {\n",
        "    \"review_id\": range(1, 21),\n",
        "    \"review\": [\n",
        "        \"처음 갔을 때는 정말 걱정이 산더미였지만 좋은 선생님과 동기들, 그리고 행정직원 분들을 만나 무사히 수료할 수 있었다. 정말 값진 시간을 보낸 것 같다.\",\n",
        "        \"너무 열심히 할 수 있어서 좋았습니다.\",\n",
        "        \"한번에 많은 것들을 배울 수 있지만 그래서 너무 힘들었어요.\",\n",
        "        \"강사님을 진짜 잘 만났다고 생각합니다. 매우 만족합니다 !!.\",\n",
        "        \"아쉬운 부분만 보완한다면 완벽한 수강이 될 거 같음.\",\n",
        "        \"감사합니다.\",\n",
        "        \"우여곡절은 많았지만 최종적으로 웹개발의 실무기술은 얻은 것 같다. mes라는 주제 또한 늦었지만 후반에 적극적 멘토기업을 통해 진솔한 피드백으로 프로젝트에 전부 녹여내진 못했지만 좀 더 mes에 대한 이해도를 높일 수 있는 계기가 되었다. 무엇보다 팀프로젝트를 메인으로 처음부터 끝까지 팀원들과 설계함에 따라 팀워크와 소통 능력을 증대할 수 있었다.\",\n",
        "        \"만족합니다.\",\n",
        "        \"기대 이하였습니다. 강의 진행 중 오류가 너무 많았습니다.\",\n",
        "        \"IT 실무에 빠르게 배우고 싶다면 추천합니다!\",\n",
        "        \"한 분이 모든 과목을 하기엔 버거웠다. 무엇 하나도 만족스럽지 못했다.\",\n",
        "        \"강사님들이 눈높이에 맞춰 강의해주시고 개인 멘토도 잘해주셔서 좋았습니다.\",\n",
        "        \"학원 측에서 많은 부분에 신경을 써주는 것이 느껴져서 좋았다.\",\n",
        "        \"시설적인 부분의 잔고장이 많아 불편했다.\",\n",
        "        \"커리큘럼이 너무 과해서 힘들었다.\",\n",
        "        \"수업이 체계적이어서 학습에 도움이 되었습니다.\",\n",
        "        \"교재가 부족해서 아쉬웠습니다.\",\n",
        "        \"실습 시간이 충분하지 않았어요.\",\n",
        "        \"강의 자료가 잘 정리되어 있어서 이해하기 쉬웠습니다.\",\n",
        "        \"전체적으로 만족스러운 경험이었습니다.\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"긍정\", \"부정\", \"긍정\", \"혼합\", \"긍정\", \"긍정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"긍정\", \"부정\", \"부정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"긍정\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# 4. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. 한국어 불용어 목록 정의 (예시로 일부 불용어 추가)\n",
        "korean_stopwords = [\"정말\", \"매우\", \"모든\", \"너무\", \"정도\", \"이\", \"그\", \"저\", \"에서\", \"그리고\"]\n",
        "\n",
        "# 6. 감성 분석 모델 정의 및 학습(TD-IDF)\n",
        "model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=korean_stopwords)),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 7. 검증 데이터에 대한 모델 평가\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 8. OpenAI API 설정\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# 9. 리뷰에 대한 설명을 생성하는 함수 정의\n",
        "def generate_explanation(review, prediction):\n",
        "    prompt = (\n",
        "        f\"휴먼 교육센터 수강생 후기 '{prediction}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# 10. 예측 및 설명 생성\n",
        "for review in X_test[:5]:  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    prediction = model.predict([review])[0]\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {prediction}\\n설명: {explanation}\\n\")"
      ],
      "metadata": {
        "id": "2aKBpZXEiPcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 교사학습\n",
        "(form C-GPT)\n",
        ">>교사학습(Supervised Learning)의 특징은 다음과 같습니다:\n",
        "\n",
        ">>레이블된 데이터 사용:\n",
        "\n",
        ">>교사학습은 입력 데이터와 그에 상응하는 정답 레이블이 포함된 데이터셋을 사용합니다. 모델은 이 레이블을 기반으로 학습하여 입력과 출력 사이의 관계를 추론합니다.\n",
        "목표 명확성:\n",
        "\n",
        ">>교사학습의 목표는 주어진 데이터를 통해 입력과 출력 간의 매핑 함수를 학습하는 것입니다. 이후 새로운 데이터에 대해서도 예측할 수 있는 능력을 갖추게 됩니다.\n",
        "모델 피드백:\n",
        "\n",
        ">>모델의 예측 결과와 실제 레이블 간의 차이를 통해 손실 함수를 계산하고, 이를 기반으로 모델이 오차를 줄이기 위해 학습이 진행됩니다.\n",
        "다양한 응용 분야:\n",
        "\n",
        ">>교사학습은 **분류(Classification)**와 회귀(Regression) 문제에 널리 사용됩니다.\n",
        "분류: 입력을 특정 범주로 분류하는 문제. 예: 스팸 이메일 분류.\n",
        "회귀: 입력에 대한 연속적인 값을 예측하는 문제. 예: 집값 예측.\n",
        "알고리즘의 종류:\n",
        "\n",
        ">>대표적인 교사학습 알고리즘으로는 로지스틱 회귀, 서포트 벡터 머신(SVM), 결정 트리, k-최근접 이웃(KNN), 신경망(Neural Networks) 등이 있습니다.\n",
        "데이터 의존성:\n",
        "\n",
        ">>교사학습의 성능은 주로 훈련 데이터의 양과 질에 의존합니다. 충분한 양의 고품질 데이터를 제공해야 좋은 성능을 기대할 수 있습니다.\n",
        "학습 과정의 투명성:\n",
        "\n",
        ">>학습 과정에서 모델은 레이블을 이용해 피드백을 받기 때문에, 학습 결과의 해석이나 성능 개선 방향을 상대적으로 명확하게 파악할 수 있습니다.\n",
        "오버피팅 위험:\n",
        "\n",
        ">>훈련 데이터에 과도하게 맞춰져 일반화 능력이 떨어지는 오버피팅의 문제가 발생할 수 있으므로, 이를 방지하기 위한 정규화나 교차 검증 같은 기법이 필요합니다.\n",
        "요약\n",
        "교사학습은 레이블이 있는 데이터를 사용해 학습하는 방식으로, 분류와 회귀 문제를 해결하는 데 사용됩니다. 목표가 명확하고 학습 과정에서 피드백을 통해 성능을 개선할 수 있지만, 데이터 의존성과 오버피팅에 주의해야 합니다."
      ],
      "metadata": {
        "id": "JLD_Gkx_nwyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 임포트\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import openai\n",
        "import re\n",
        "\n",
        "# 2. 영화 리뷰 데이터 생성 및 데이터프레임 만들기 (데이터 수 증가)\n",
        "data = {\n",
        "    \"review_id\": range(1, 101),\n",
        "    \"review\": [\n",
        "        \"이 영화는 정말 환상적이었어요! 스토리가 매우 흡입력이 있었습니다.\",\n",
        "        \"영화의 모든 순간이 싫었어요. 연기가 형편없었어요.\",\n",
        "        \"비주얼은 멋졌지만, 줄거리가 부족했어요.\",\n",
        "        \"즐거운 관람이었고, 멋진 배우들의 연기였습니다.\",\n",
        "        \"제 취향이 아니었어요, 너무 느리고 지루했어요.\",\n",
        "        \"정말 감동적인 이야기였고, 눈물이 났어요.\",\n",
        "        \"대사가 너무 뻔했어요. 예측 가능한 전개였어요.\",\n",
        "        \"음악이 정말 좋았고, 분위기를 잘 살렸어요.\",\n",
        "        \"기대 이하였습니다. 더 잘 만들 수 있었을 것 같아요.\",\n",
        "        \"배우들의 케미가 좋았고, 재미있게 봤어요.\",\n",
        "        \"액션 장면이 너무 많아서 지루했어요.\",\n",
        "        \"스토리가 탄탄하고 캐릭터가 매력적이었어요.\",\n",
        "        \"너무 어두운 내용이라서 기분이 안 좋았어요.\",\n",
        "        \"코미디 요소가 많아서 웃으면서 봤습니다.\",\n",
        "        \"전반적으로 지루하고 감정선이 약했어요.\",\n",
        "        \"화면이 아름답고, 연출이 훌륭했어요.\",\n",
        "        \"캐릭터들이 매력적이지 않아서 몰입이 안 됐어요.\",\n",
        "        \"끝까지 긴장감이 넘쳐서 손에 땀을 쥐게 했어요.\",\n",
        "        \"스토리가 엉성하고, 끝이 허무했어요.\",\n",
        "        \"다시 보고 싶을 만큼 좋은 영화였습니다.\"\n",
        "    ] * 5,  # 데이터 수를 늘리기 위해 5배 반복 = 데이터 증강기법. 학습사이즈가 너무 작으면 결과가 안나온다\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"부정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\"\n",
        "    ] * 5  # 데이터 수를 늘리기 위해 5배 반복 = 에폭 반복하는것과 같은 결( 반복 + 각도를 틀어준다. )\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 3. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. 텍스트 데이터 전처리\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # 특수 문자 제거\n",
        "    return text.split()\n",
        "\n",
        "# 전처리된 텍스트 리스트로 변환\n",
        "sentences = [preprocess_text(review) for review in X_train]\n",
        "\n",
        "# 5. Word2Vec 모델 학습 (파라미터 조정: 벡터 크기 및 학습 횟수 증가)\n",
        "word2vec_model = Word2Vec(sentences, vector_size=200, window=5, min_count=1, workers=4, epochs=20)\n",
        "# window=5 한번에 작업하는 숫자 = batch size. / epochs = 한 바퀴 다 도는 것\n",
        "# 백터의 행렬= 텐서 .-> 텐서 플로우 (텐서층)\n",
        "\n",
        "# 6. 리뷰를 Word2Vec 벡터로 변환하는 함수 정의\n",
        "def get_average_word2vec(review, model, vector_size=200):\n",
        "    words = preprocess_text(review)\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# 7. 학습 및 테스트 데이터를 벡터화\n",
        "X_train_vectors = np.array([get_average_word2vec(review, word2vec_model) for review in X_train])\n",
        "X_test_vectors = np.array([get_average_word2vec(review, word2vec_model) for review in X_test])\n",
        "\n",
        "# 8. 분류 모델 정의 및 학습 (RandomForestClassifier 사용)\n",
        "classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# 9. 검증 데이터에 대한 모델 평가\n",
        "y_pred = classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # accuracy= 분류 <=> 회귀 (R-squared)\n",
        "\n",
        "# 10. OpenAI API 설정 (API 키를 반드시 설정하세요)\n",
        "openai.api_key = \"\n",
        "\n",
        "# 11. 리뷰에 대한 설명을 생성하는 함수 정의\n",
        "def generate_explanation(review, prediction):\n",
        "    prompt = (\n",
        "        f\"다음 영화 리뷰의 감성은 '{prediction}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# 12. 예측 및 설명 생성\n",
        "for review in X_test[:5]:  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    prediction = classifier.predict([get_average_word2vec(review, word2vec_model)])[0]\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {prediction}\\n설명: {explanation}\\n\")"
      ],
      "metadata": {
        "id": "oFBZldjpcCLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 응용\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import openai\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 2. 영화 리뷰 데이터 생성 및 데이터프레임 만들기 (데이터 수 증가)\n",
        "original_data = {\n",
        "    \"review_id\": range(1, 21),\n",
        "    \"review\": [\n",
        "        \"처음 갔을 때는 정말 걱정이 산더미였지만 좋은 선생님과 동기들, 그리고 행정직원 분들을 만나 무사히 수료할 수 있었다. 정말 값진 시간을 보낸 것 같다.\",\n",
        "        \"너무 열심히 할 수 있어서 좋았습니다.\",\n",
        "        \"한번에 많은 것들을 배울 수 있지만 그래서 너무 힘들었어요.\",\n",
        "        \"강사님을 진짜 잘 만났다고 생각합니다. 매우 만족합니다 !!.\",\n",
        "        \"아쉬운 부분만 보완한다면 완벽한 수강이 될 거 같음.\",\n",
        "        \"감사합니다.\",\n",
        "        \"우여곡절은 많았지만 최종적으로 웹개발의 실무기술은 얻은 것 같다. mes라는 주제 또한 늦었지만 후반에 적극적 멘토기업을 통해 진솔한 피드백으로 프로젝트에 전부 녹여내진 못했지만 좀 더 mes에 대한 이해도를 높일 수 있는 계기가 되었다. 무엇보다 팀프로젝트를 메인으로 처음부터 끝까지 팀원들과 설계함에 따라 팀워크와 소통 능력을 증대할 수 있었다.\",\n",
        "        \"만족합니다.\",\n",
        "        \"기대 이하였습니다. 강의 진행 중 오류가 너무 많았습니다.\",\n",
        "        \"IT 실무에 빠르게 배우고 싶다면 추천합니다!\",\n",
        "        \"한 분이 모든 과목을 하기엔 버거웠다. 무엇 하나도 만족스럽지 못했다.\",\n",
        "        \"강사님들이 눈높이에 맞춰 강의해주시고 개인 멘토도 잘해주셔서 좋았습니다.\",\n",
        "        \"학원 측에서 많은 부분에 신경을 써주는 것이 느껴져서 좋았다.\",\n",
        "        \"시설적인 부분의 잔고장이 많아 불편했다.\",\n",
        "        \"커리큘럼이 너무 과해서 힘들었다.\",\n",
        "        \"수업이 체계적이어서 학습에 도움이 되었습니다.\",\n",
        "        \"교재가 부족해서 아쉬웠습니다.\",\n",
        "        \"실습 시간이 충분하지 않았어요.\",\n",
        "        \"강의 자료가 잘 정리되어 있어서 이해하기 쉬웠습니다.\",\n",
        "        \"전체적으로 만족스러운 경험이었습니다.\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"긍정\", \"부정\", \"긍정\", \"혼합\", \"긍정\", \"긍정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"긍정\", \"부정\", \"부정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"긍정\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 데이터 증강: 리스트를 5배로 반복하여 100개의 데이터 생성\n",
        "augmented_data = {\n",
        "    \"review_id\": range(1, 101),  # 1부터 100까지\n",
        "    \"review\": original_data[\"review\"] * 5,  # 20 * 5 = 100\n",
        "    \"sentiment\": original_data[\"sentiment\"] * 5  # 20 * 5 = 100\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(augmented_data)\n",
        "\n",
        "# 데이터프레임 생성 확인\n",
        "print(\"review_id 길이:\", len(df['review_id']))    # 100\n",
        "print(\"review 길이:\", len(df['review']))          # 100\n",
        "print(\"sentiment 길이:\", len(df['sentiment']))    # 100\n",
        "\n",
        "# 3. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. 텍스트 데이터 전처리\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # 특수 문자 제거\n",
        "    return text.split()\n",
        "\n",
        "# 전처리된 텍스트 리스트로 변환\n",
        "sentences = [preprocess_text(review) for review in X_train]\n",
        "\n",
        "# 5. Word2Vec 모델 학습 (파라미터 조정: 벡터 크기 및 학습 횟수 증가)\n",
        "word2vec_model = Word2Vec(sentences, vector_size=200, window=5, min_count=1, workers=4, epochs=20)\n",
        "\n",
        "# 6. 리뷰를 Word2Vec 벡터로 변환하는 함수 정의\n",
        "def get_average_word2vec(review, model, vector_size=200):\n",
        "    words = preprocess_text(review)\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# 7. 학습 및 테스트 데이터를 벡터화\n",
        "X_train_vectors = np.array([get_average_word2vec(review, word2vec_model) for review in X_train])\n",
        "X_test_vectors = np.array([get_average_word2vec(review, word2vec_model) for review in X_test])\n",
        "\n",
        "# 8. 분류 모델 정의 및 학습 (RandomForestClassifier 사용)\n",
        "classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# 9. 검증 데이터에 대한 모델 평가\n",
        "y_pred = classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # 분류 문제에서 정확도 평가\n",
        "\n",
        "# 10. OpenAI API 설정 (API 키를 반드시 설정하세요)\n",
        "openai.api_key = \"\"  # 여기에 실제 API 키를 입력하세요.\n",
        "\n",
        "# 11. 리뷰에 대한 설명을 생성하는 함수 정의\n",
        "def generate_explanation(review, prediction):\n",
        "    prompt = (\n",
        "        f\"다음 영화 리뷰의 감성은 '{prediction}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        return f\"설명을 생성하는 중 오류가 발생했습니다: {e}\"\n",
        "\n",
        "# 12. 예측 및 설명 생성\n",
        "for review in X_test[:5]:  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    prediction = classifier.predict([get_average_word2vec(review, word2vec_model)])[0]\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {prediction}\\n설명: {explanation}\\n\")\n"
      ],
      "metadata": {
        "id": "0mY0aD6dtTZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT로 학습"
      ],
      "metadata": {
        "id": "X2OVMT5IcEYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bert = 트랜스포머의 인코더"
      ],
      "metadata": {
        "id": "IH-2RoEht2cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 임포트\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "Xrx4MB_8cFZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 임포트\n",
        "!pip install transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import openai\n",
        "import re\n",
        "\n",
        "# 2. 영화 리뷰 데이터 생성 및 데이터프레임 만들기 (데이터 수 증가)\n",
        "data = {\n",
        "    \"review_id\": range(1, 101),\n",
        "    \"review\": [\n",
        "        \"이 영화는 정말 환상적이었어요! 스토리가 매우 흡입력이 있었습니다.\",\n",
        "        \"영화의 모든 순간이 싫었어요. 연기가 형편없었어요.\",\n",
        "        \"비주얼은 멋졌지만, 줄거리가 부족했어요.\",\n",
        "        \"즐거운 관람이었고, 멋진 배우들의 연기였습니다.\",\n",
        "        \"제 취향이 아니었어요, 너무 느리고 지루했어요.\",\n",
        "        \"정말 감동적인 이야기였고, 눈물이 났어요.\",\n",
        "        \"대사가 너무 뻔했어요. 예측 가능한 전개였어요.\",\n",
        "        \"음악이 정말 좋았고, 분위기를 잘 살렸어요.\",\n",
        "        \"기대 이하였습니다. 더 잘 만들 수 있었을 것 같아요.\",\n",
        "        \"배우들의 케미가 좋았고, 재미있게 봤어요.\",\n",
        "        \"액션 장면이 너무 많아서 지루했어요.\",\n",
        "        \"스토리가 탄탄하고 캐릭터가 매력적이었어요.\",\n",
        "        \"너무 어두운 내용이라서 기분이 안 좋았어요.\",\n",
        "        \"코미디 요소가 많아서 웃으면서 봤습니다.\",\n",
        "        \"전반적으로 지루하고 감정선이 약했어요.\",\n",
        "        \"화면이 아름답고, 연출이 훌륭했어요.\",\n",
        "        \"캐릭터들이 매력적이지 않아서 몰입이 안 됐어요.\",\n",
        "        \"끝까지 긴장감이 넘쳐서 손에 땀을 쥐게 했어요.\",\n",
        "        \"스토리가 엉성하고, 끝이 허무했어요.\",\n",
        "        \"다시 보고 싶을 만큼 좋은 영화였습니다.\"\n",
        "    ] * 5,  # 데이터 수를 늘리기 위해 5배 반복\n",
        "    \"sentiment\": [\n",
        "        \"긍정\", \"부정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\",\n",
        "        \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\", \"부정\", \"긍정\"\n",
        "    ] * 5  # 데이터 수를 늘리기 위해 5배 반복\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 3. 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 5. BERT 토크나이저 설정\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 6. 커스텀 Dataset 클래스 정의\n",
        "class MovieReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, tokenizer, max_len=128):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 7. Dataset과 DataLoader 설정\n",
        "train_dataset = MovieReviewDataset(X_train.to_numpy(), y_train_encoded, tokenizer)\n",
        "test_dataset = MovieReviewDataset(X_test.to_numpy(), y_test_encoded, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10)\n",
        "\n",
        "# 8. BERT 모델 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
        "model = model.to(device)\n",
        "\n",
        "# 9. 옵티마이저와 스케줄러 설정\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * 3  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# 10. 학습 루프\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# 11. 학습 실행\n",
        "for epoch in range(3):\n",
        "    print(f'Epoch {epoch + 1}/{3}')\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler\n",
        "    )\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "# 12. 검증 데이터 평가\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    reviews, predictions, true_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            reviews.extend(d[\"review_text\"])\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "    return reviews, predictions, true_labels\n",
        "\n",
        "# 13. 예측 결과와 설명 생성\n",
        "openai.api_key = \"\"\n",
        "\n",
        "def generate_explanation(review, prediction):\n",
        "    sentiment = label_encoder.inverse_transform([prediction])[0]\n",
        "    prompt = (\n",
        "        f\"다음 영화 리뷰의 감성은 '{sentiment}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# 14. 테스트 데이터에 대한 예측 및 설명 생성\n",
        "reviews, predictions, true_labels = eval_model(model, test_loader, device)\n",
        "\n",
        "for i in range(5):  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    review = reviews[i]\n",
        "    prediction = predictions[i].cpu().item()\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {label_encoder.inverse_transform([prediction])[0]}\\n설명: {explanation}\\n\")"
      ],
      "metadata": {
        "id": "__oGVMfVcGUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "6fiiUMWey4Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Java 설치 및 konlpy 설치\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install konlpy\n",
        "\n",
        "# imbalanced-learn 설치\n",
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "zvv6zNS9y_HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 설치 및 임포트\n",
        "!pip install transformers torch scikit-learn imbalanced-learn\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install konlpy\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import openai\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 2. 영화 리뷰 데이터 생성 및 데이터프레임 만들기 (데이터 수 증가)\n",
        "data = {\n",
        "    \"review_id\": range(1, 101),\n",
        "    \"review\": [\n",
        "        \"이 영화는 정말 환상적이었어요! 스토리가 매우 흡입력이 있었습니다.\",\n",
        "        \"영화의 모든 순간이 싫었어요. 보는 사람까지 우울해졌어요.\",\n",
        "        \"비주얼은 멋졌지만, 줄거리가 부족했어요.\",\n",
        "        \"즐거운 관람이었고, 멋진 배우들의 연기였습니다.\",\n",
        "        \"제 취향이 아니었어요, 너무 어둡고 음침했어요.\",\n",
        "        \"정말 감동적인 이야기였고, 눈물이 났어요.\",\n",
        "        \"옆에서 보던 사람이 코를 골아서 짜증났어요.\",\n",
        "        \"음악이 정말 좋았고, 분위기를 잘 살렸어요.\",\n",
        "        \"기대 이하였습니다. 연출 잡아와.\",\n",
        "        \"배우들의 케미가 좋았고, 재미있게 봤어요.\",\n",
        "        \"액션 장면이 너무 많아서 지루했어요.\",\n",
        "        \"스토리가 탄탄하고 캐릭터가 매력적이었어요.\",\n",
        "        \"너무 어두운 내용이라서 기분이 안 좋았어요.\",\n",
        "        \"코미디 요소가 많아서 웃으면서 봤습니다.\",\n",
        "        \"전반적으로 지루하고 감정선이 약했어요.\",\n",
        "        \"화면이 아름답고, 연출이 훌륭했어요.\",\n",
        "        \"캐릭터들이 매력적이지 않아서 몰입이 안 됐어요.\",\n",
        "        \"끝까지 긴장감이 넘쳐서 손에 땀을 쥐게 했어요.\",\n",
        "        \"스토리가 엉성하고, 끝이 허무했어요.\",\n",
        "        \"다시 보고 싶을 만큼 좋은 영화였습니다.\"\n",
        "    ] * 5,  # 데이터 수를 늘리기 위해 5배 반복\n",
        "    \"sentiment\": [\n",
        "        \"기쁨\", \"실망\", \"화남\", \"기쁨\", \"실망\",\n",
        "        \"기쁨\", \"때리고싶음\", \"기쁨\", \"때리고싶음\", \"기쁨\",\n",
        "        \"화남\", \"기쁨\", \"화남\", \"웃김\", \"짜증남\",\n",
        "        \"기쁨\", \"짜증남\", \"기쁨\", \"화남\", \"기쁨\"\n",
        "    ] * 5  # 데이터 수를 늘리기 위해 5배 반복\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 데이터프레임 생성 확인\n",
        "print(\"review_id 길이:\", len(df['review_id']))    # 100\n",
        "print(\"review 길이:\", len(df['review']))          # 100\n",
        "print(\"sentiment 길이:\", len(df['sentiment']))    # 100\n",
        "\n",
        "# 3. 학습 데이터 증강: 오버샘플링 (필요 시)\n",
        "# 현재 데이터는 100개로 적으므로, 오버샘플링을 통해 클래스 균형을 맞출 수 있습니다.\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(\n",
        "    df[['review']],\n",
        "    df['sentiment']\n",
        ")\n",
        "df_resampled = pd.DataFrame({\n",
        "    \"review_id\": range(1, len(X_resampled)+1),\n",
        "    \"review\": X_resampled['review'],\n",
        "    \"sentiment\": y_resampled\n",
        "})\n",
        "\n",
        "# 학습 및 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_resampled['review'],\n",
        "    df_resampled['sentiment'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_resampled['sentiment']\n",
        ")\n",
        "\n",
        "# 4. 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "print(\"레이블 클래스:\", label_encoder.classes_)  # 라벨 클래스 확인\n",
        "\n",
        "# 5. BERT 토크나이저 설정\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 6. 커스텀 Dataset 클래스 정의\n",
        "class MovieReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, tokenizer, okt, max_len=128):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.okt = okt\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        # 전처리: 특수 문자 제거 및 형태소 분석을 통한 토큰화\n",
        "        review = re.sub(r\"[^\\w\\s]\", \"\", review)\n",
        "        tokens = self.okt.morphs(review)\n",
        "        cleaned_review = \" \".join(tokens)\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            cleaned_review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 7. Dataset과 DataLoader 설정\n",
        "train_dataset = MovieReviewDataset(X_train.to_numpy(), y_train_encoded, tokenizer, okt)\n",
        "test_dataset = MovieReviewDataset(X_test.to_numpy(), y_test_encoded, tokenizer, okt)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10)\n",
        "\n",
        "# 8. BERT 모델 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-multilingual-cased',\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# 9. 옵티마이저와 스케줄러 설정\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * 3  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# 클래스 가중치 계산 (클래스 불균형 처리)\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class_counts = np.bincount(y_train_encoded)\n",
        "class_weights = 1. / class_counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# 10. 학습 루프\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, loss_fn):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# 11. 학습 실행\n",
        "for epoch in range(3):\n",
        "    print(f'Epoch {epoch + 1}/3')\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        loss_fn\n",
        "    )\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "# 12. 검증 데이터 평가\n",
        "def eval_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    reviews, predictions, true_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            reviews.extend(d[\"review_text\"])\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "    return reviews, predictions, true_labels\n",
        "\n",
        "# 13. 예측 결과와 설명 생성\n",
        "openai.api_key = \"\"  # 여기에 실제 API 키를 입력하세요.\n",
        "\n",
        "def generate_explanation(review, prediction):\n",
        "    sentiment = label_encoder.inverse_transform([prediction])[0]\n",
        "    prompt = (\n",
        "        f\"다음 영화 리뷰의 감성은 '{sentiment}'입니다. 리뷰: \\\"{review}\\\". \"\n",
        "        \"이 리뷰가 왜 이런 감성으로 분류되었는지 설명해 주세요.\"\n",
        "    )\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        return f\"설명을 생성하는 중 오류가 발생했습니다: {e}\"\n",
        "\n",
        "# 14. 테스트 데이터에 대한 예측 및 설명 생성\n",
        "reviews, predictions, true_labels = eval_model(model, test_loader, device)\n",
        "\n",
        "# 모델 성능 평가\n",
        "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))\n",
        "\n",
        "for i in range(5):  # 일부 테스트 데이터에 대해 예제 실행\n",
        "    review = reviews[i]\n",
        "    prediction = predictions[i].cpu().item()\n",
        "    explanation = generate_explanation(review, prediction)\n",
        "    print(f\"리뷰: {review}\\n예측: {label_encoder.inverse_transform([prediction])[0]}\\n설명: {explanation}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KzV4pnGiyCZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text 분류 실습 2\n",
        "\n",
        "BERT 모델을 사용하여 문서의도를 예측한 후, GPT-3.5로 설명 생성\n",
        "\n",
        "1. **BERT 모델을 사용하여 문서의도 분류**: 각 문서에 대해 `정보 요청`, `명령`, `질문`, `감사`, `불만` 중 하나의 의도를 예측합니다.\n",
        "2. **GPT-3.5로 예측에 대한 설명 생성**: 예측된 의도를 바탕으로 GPT-3.5에게 왜 해당 문서가 특정 의도로 분류되었는지 설명하도록 요청합니다.\n",
        "\n",
        "아래는 BERT 모델을 사용하여 문서의도를 예측하고, GPT-3.5를 사용해 설명을 생성하는 코드입니다."
      ],
      "metadata": {
        "id": "0yNKP8k9uHC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 임포트\n",
        "!pip install openai==0.28\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "xxCzZtGzuNT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : 데이터셋을 학습하고 모델을 저장"
      ],
      "metadata": {
        "id": "LWAkFhkquOzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. 문서 의도 예제 데이터 생성\n",
        "data = {\n",
        "    \"document\": [\n",
        "        \"어디에서 이 제품을 구입할 수 있나요?\",\n",
        "        \"다음 주까지 이 작업을 완료하세요.\",\n",
        "        \"이 문제를 어떻게 해결할 수 있나요?\",\n",
        "        \"도와주셔서 정말 감사합니다.\",\n",
        "        \"서비스가 형편없습니다. 다시 이용하지 않을 것입니다.\",\n",
        "        \"이 제품은 어디서 찾을 수 있나요?\",\n",
        "        \"이 기능의 사용 방법을 알려주세요.\",\n",
        "        \"새로운 버전을 어디서 다운로드할 수 있나요?\",\n",
        "        \"이 문제에 대한 자세한 정보가 필요합니다.\",\n",
        "        \"이 제품의 사용 설명서를 어디서 찾을 수 있나요?\",\n",
        "        \"이 일을 빨리 끝내 주세요.\",\n",
        "        \"모든 보고서를 내일까지 제출하세요.\",\n",
        "        \"이 작업을 완료하는 데 시간이 얼마나 걸리나요?\",\n",
        "        \"지금 바로 이 작업을 시작하세요.\",\n",
        "        \"이 요청을 지체 없이 처리해 주세요.\",\n",
        "        \"이 오류를 어떻게 고칠 수 있나요?\",\n",
        "        \"이 기능은 어떻게 활성화하나요?\",\n",
        "        \"이 문제를 해결하는 다른 방법이 있나요?\",\n",
        "        \"왜 이런 현상이 발생하는지 알려줄 수 있나요?\",\n",
        "        \"이 상황에서 어떤 조치를 취해야 하나요?\",\n",
        "        \"당신의 지원에 깊이 감사드립니다.\",\n",
        "        \"문제를 신속히 해결해 주셔서 감사합니다.\",\n",
        "        \"귀하의 도움이 큰 도움이 되었습니다.\",\n",
        "        \"적극적인 지원에 대해 감사드립니다.\",\n",
        "        \"이 서비스는 매우 실망스럽습니다.\",\n",
        "        \"제품 품질이 기대 이하입니다.\",\n",
        "        \"고객 지원이 너무 느립니다.\",\n",
        "        \"이 서비스는 전혀 만족스럽지 않습니다.\",\n",
        "        \"다시는 이 서비스를 이용하지 않을 것입니다.\",\n",
        "        \"배송이 너무 늦어 불만입니다.\",\n",
        "        \"이 서비스의 가격은 얼마인가요?\",\n",
        "        \"어떤 기능이 제공되나요?\",\n",
        "        \"이 제품의 사양을 알려주세요.\",\n",
        "        \"이 제품은 어떤 용도로 사용되나요?\",\n",
        "        \"이 문제를 해결할 수 있는 사람은 누구인가요?\",\n",
        "        \"이 기능에 대해 자세히 설명해 주세요.\",\n",
        "        \"이 문제의 원인을 찾는 방법은 무엇인가요?\",\n",
        "        \"이 항목에 대한 세부 정보가 필요합니다.\",\n",
        "        \"지원해 주셔서 감사합니다.\",\n",
        "        \"귀하의 도움에 진심으로 감사드립니다.\",\n",
        "        \"여러분의 노고에 감사드립니다.\",\n",
        "        \"서비스가 개선되어 정말 기쁩니다.\",\n",
        "        \"고객 서비스가 너무 불친절합니다.\",\n",
        "        \"제품의 품질이 너무 떨어집니다.\",\n",
        "        \"이 문제로 인해 큰 불편을 겪었습니다.\",\n",
        "        \"서비스 이용에 매우 실망했습니다.\",\n",
        "        \"내일까지 이 작업을 끝내세요.\",\n",
        "        \"즉시 이 문제를 해결하세요.\",\n",
        "        \"이 일을 가능한 한 빨리 처리해 주세요.\",\n",
        "        \"이 요청을 신속히 이행해 주세요.\",\n",
        "        \"왜 이런 오류가 발생하는지 설명해 주세요.\",\n",
        "        \"이 문제를 해결하기 위한 단계를 알려 주세요.\",\n",
        "        \"이 기능을 사용할 수 없는 이유는 무엇인가요?\",\n",
        "        \"어떤 방식으로 접근해야 할까요?\",\n",
        "        \"정말 친절하게 대해 주셔서 감사합니다.\",\n",
        "        \"빠른 처리에 감사드립니다.\",\n",
        "        \"문제를 해결해 주셔서 감사합니다.\",\n",
        "        \"도움을 주셔서 매우 기쁩니다.\",\n",
        "        \"서비스가 너무 느립니다. 개선이 필요합니다.\",\n",
        "        \"배송이 너무 지연됩니다.\",\n",
        "        \"제품의 품질이 기대 이하입니다.\",\n",
        "        \"고객 지원이 만족스럽지 않습니다.\",\n",
        "        \"이 제품은 왜 이렇게 비싼가요?\",\n",
        "        \"어떤 보증이 제공되나요?\",\n",
        "        \"이 서비스는 어떻게 이용하나요?\",\n",
        "        \"다른 대안은 무엇인가요?\",\n",
        "        \"어떻게 하면 이 문제를 더 빨리 해결할 수 있을까요?\",\n",
        "        \"서비스 품질이 정말 훌륭합니다. 고맙습니다.\",\n",
        "        \"고객 서비스가 매우 친절했습니다.\",\n",
        "        \"제품이 기대 이상으로 좋았습니다.\",\n",
        "        \"이 기능을 추가해 주셔서 감사합니다.\",\n",
        "        \"이용해 주셔서 감사합니다.\",\n",
        "        \"서비스가 정말 좋았습니다.\",\n",
        "        \"이 문제를 더 이상 겪고 싶지 않습니다.\",\n",
        "        \"다시는 이 제품을 구매하지 않을 것입니다.\",\n",
        "        \"너무 실망스럽습니다.\",\n",
        "        \"제품이 제 기대에 미치지 못했습니다.\",\n",
        "        \"이 문제에 대해 불만을 제기하고 싶습니다.\",\n",
        "        \"어떤 방법으로 환불을 받을 수 있나요?\",\n",
        "        \"환불 절차를 안내해 주세요.\",\n",
        "        \"서비스를 개선해 주셔서 감사합니다.\",\n",
        "        \"이 문제를 해결해 주셔서 기쁩니다.\",\n",
        "        \"다른 질문이 있습니다. 답변해 주세요.\",\n",
        "        \"추가 정보가 필요합니다.\",\n",
        "        \"이 상황에서 어떻게 해야 하나요?\",\n",
        "        \"다음 단계를 안내해 주세요.\",\n",
        "        \"이 제품을 추천해 주시겠어요?\",\n",
        "        \"서비스 이용에 대해 추가 질문이 있습니다.\",\n",
        "        \"문제가 여전히 해결되지 않았습니다.\",\n",
        "        \"이 문제를 해결하는 데 얼마나 걸릴까요?\",\n",
        "        \"어떻게 하면 이 상황을 개선할 수 있을까요?\",\n",
        "        \"이 기능에 대해 더 알고 싶습니다.\"\n",
        "    ],\n",
        "    \"intent\": [\n",
        "        \"정보 요청\", \"명령\", \"질문\", \"감사\", \"불만\",\n",
        "        \"정보 요청\", \"정보 요청\", \"명령\", \"질문\", \"정보 요청\",\n",
        "        \"명령\", \"명령\", \"질문\", \"명령\", \"명령\",\n",
        "        \"질문\", \"질문\", \"질문\", \"질문\", \"질문\",\n",
        "        \"감사\", \"감사\", \"감사\", \"감사\", \"불만\",\n",
        "        \"불만\", \"불만\", \"불만\", \"불만\", \"불만\",\n",
        "        \"정보 요청\", \"정보 요청\", \"정보 요청\", \"정보 요청\", \"질문\",\n",
        "        \"질문\", \"정보 요청\", \"정보 요청\", \"감사\", \"감사\",\n",
        "        \"감사\", \"감사\", \"불만\", \"불만\", \"불만\",\n",
        "        \"불만\", \"명령\", \"명령\", \"명령\", \"명령\",\n",
        "        \"질문\", \"질문\", \"질문\", \"질문\", \"감사\",\n",
        "        \"감사\", \"감사\", \"감사\", \"불만\", \"불만\",\n",
        "        \"불만\", \"불만\", \"정보 요청\", \"정보 요청\", \"정보 요청\",\n",
        "        \"정보 요청\", \"질문\", \"질문\", \"질문\", \"질문\",\n",
        "        \"감사\", \"감사\", \"감사\", \"감사\", \"불만\",\n",
        "        \"불만\", \"불만\", \"불만\", \"불만\", \"불만\",\n",
        "        \"불만\", \"불만\", \"불만\", \"불만\", \"불만\",\n",
        "        \"정보 요청\", \"정보 요청\", \"질문\", \"질문\", \"질문\",\n",
        "        \"질문\", \"정보 요청\", \"질문\", \"질문\", \"질문\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# 2. 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(data['intent'])\n",
        "\n",
        "# 3. BERT 토크나이저 설정\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 4. 커스텀 Dataset 클래스 정의\n",
        "class IntentDataset(Dataset):\n",
        "    def __init__(self, documents, labels, tokenizer, max_len=64):\n",
        "        self.documents = documents\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        document = str(self.documents[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            document,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'document_text': document,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 5. Dataset과 DataLoader 설정\n",
        "dataset = IntentDataset(data['document'], labels_encoded, tokenizer)\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 6. BERT 모델 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
        "model = model.to(device)\n",
        "\n",
        "# 7. 옵티마이저 및 학습 설정\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "# 8. 모델 학습\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 9. 모델 저장\n",
        "model_save_path = \"bert_intent_model.pth\" # pth는 값만 저장됨\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"모델이 저장되었습니다: {model_save_path}\")\n",
        "\n",
        "# 10. 모델 평가\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        all_predictions.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f\"모델 정확도: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "iQwMItL3uTw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 (분류): 저장된 모델을 로딩하고 LLM(GPT-3.5)에 Prompt를 전송하여 설명문장을 응답받아 문서분류와 설명문장을 완성하는 코드"
      ],
      "metadata": {
        "id": "3pQQl1j2uY23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import openai\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. 레이블 인코딩 초기화 (이전 학습 시 사용한 것과 동일하게 설정)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit([\n",
        "    \"정보 요청\", \"명령\", \"질문\", \"감사\", \"불만\"\n",
        "])\n",
        "\n",
        "# 2. 모델 및 토크나이저 로딩\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))\n",
        "model.load_state_dict(torch.load(\"bert_intent_model.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# 3. 예측 함수 정의\n",
        "def predict_intent(model, tokenizer, text, device):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, prediction = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "    return prediction.item()\n",
        "\n",
        "# 4. GPT-3.5를 통한 설명 생성 함수 정의\n",
        "openai.api_key = \"\"\n",
        "\n",
        "def generate_explanation(document, prediction):\n",
        "    intent = label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    다음 문서의 의도는 '{intent}'입니다. 문서: \"{document}\".\n",
        "    이 문서가 왜 이런 의도로 분류되었는지 설명해 주세요.\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# 5. 테스트 문장에 대한 예측 및 설명 생성\n",
        "test_sentences = [\n",
        "    \"이 제품에 대한 추가 정보를 제공해 주세요.\",\n",
        "    \"내일까지 이 작업을 끝내세요.\",\n",
        "    \"이 문제를 어떻게 해결할 수 있나요?\",\n",
        "    \"도움을 주셔서 정말 감사합니다.\",\n",
        "    \"서비스가 전혀 만족스럽지 않습니다.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # 의도 예측\n",
        "    prediction = predict_intent(model, tokenizer, sentence, device)\n",
        "\n",
        "    # GPT-3.5를 통한 설명 생성\n",
        "    explanation = generate_explanation(sentence, prediction)\n",
        "    print(f\"문서: {sentence}\\n예측된 의도: {label_encoder.inverse_transform([prediction])[0]}\\n설명: {explanation}\\n\")"
      ],
      "metadata": {
        "id": "rxpYqFy3ua7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> app.py에 올리게 되는건 학습기가 아니라 step2 분류기."
      ],
      "metadata": {
        "id": "9A2HOsgb5Wpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 설명:\n",
        "\n",
        "1. **데이터 생성**: 문서의도 분석을 위한 예시 데이터셋을 생성합니다.\n",
        "2. **BERT 모델을 사용한 예측**: 각 문서에 대해 `정보 요청`, `명령`, `질문`, `감사`, `불만` 중 하나의 의도를 예측합니다.\n",
        "3. **GPT-3.5를 사용한 설명 생성**: 예측된 의도를 바탕으로 GPT-3.5에게 해당 문서가 왜 그 의도로 분류되었는지 설명을 요청합니다.\n",
        "4. **결과 출력**: 문서의 내용, 예측된 의도, 그리고 설명을 출력합니다.\n",
        "\n",
        "이렇게 하면 사용자는 모델이 왜 특정 문서를 특정 의도로 분류했는지를 더 잘 이해할 수 있습니다."
      ],
      "metadata": {
        "id": "gBhx8f_DudvH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpuDf05C5WUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}