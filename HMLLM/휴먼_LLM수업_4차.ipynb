{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyONAWX3oqWCdrlvADg+zaL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yul77/Review/blob/main/HMLLM/%ED%9C%B4%EB%A8%BC_LLM%EC%88%98%EC%97%85_4%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Part1.day4 : Seq2seq Modeling"
      ],
      "metadata": {
        "id": "ETFhcOIVOnSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Modeling\n",
        "\n",
        "**Seq2Seq(Sequence-to-Sequence)** 모델은 하나의 시퀀스를 입력으로 받아 다른 시퀀스를 출력하는 **신경망 모델**입니다. 주로 자연어 처리(NLP)에서 사용되며, 번역, 요약, 대화 생성 등 다양한 작업에서 중요한 역할을 합니다. Seq2Seq 모델은 입력 시퀀스를 고정된 길이의 **벡터 표현**으로 인코딩한 후, 디코딩하여 출력 시퀀스를 생성합니다.\n",
        "\n",
        ">> 시큐투시퀀스.\n",
        ">> 문자-> 숫자로 바꿀때 원핫 인코딩과 임베딩을 사용하는데 원핫인코딩은 성능이 너무 떨어지므로 임베딩 사용 권장. 임베딩(워드 투 벡터 / 버트)\n",
        ">> 공부할때 제일 중요한건 RAG. 성능을 좌우한다.\n",
        "\n",
        "\n",
        "\n",
        "### Seq2Seq의 주요 구성 요소\n",
        "\n",
        "1. **인코더(Encoder)**\n",
        "    - 인코더는 입력 시퀀스를 받아 이를 **고차원 벡터**로 변환합니다. 이 벡터는 입력 시퀀스의 중요한 정보를 압축하여 담고 있습니다.\n",
        "    - 인코더는 일반적으로 **RNN(Recurrent Neural Network)**, **LSTM(Long Short-Term Memory)**, **GRU(Gated Recurrent Unit)** 또는 **Transformer** 기반의 구조로 구성됩니다.\n",
        "    >> 시퀀스 안에 트랜스 포머가 들어가는것. 다만 트랜스포머는 인코더가 버트.디코더가 지피티 인것.\n",
        "\n",
        "    >> RNN < LSTM < GRU\n",
        "    - 예를 들어, 영어 문장을 인코더에 입력하면 이 문장의 의미를 담고 있는 고정된 크기의 벡터로 변환됩니다.\n",
        "2. **디코더(Decoder)**\n",
        "    - 디코더는 인코더에서 전달받은 벡터 표현을 입력으로 받아 **출력 시퀀스**를 생성합니다.\n",
        "    - 디코더 역시 RNN, LSTM, GRU 또는 Transformer 구조로 구성될 수 있으며, 이 구조는 인코더와 연결되어 있습니다.\n",
        "    >> Seq2Seq는 아버지/ 트랜스포머는 아들이라고 보면 됨.\n",
        "    >> 한가지 차이점은 트랜스포머는 생성형 모델이 들어가있음.(= gpt)\n",
        "    - 출력은 단어 단위로 생성되며, 이전에 생성된 단어들을 참고하여 다음 단어를 예측합니다.\n",
        "\n",
        "### Seq2Seq 모델의 학습 과정\n",
        "\n",
        "1. **학습 데이터 준비**\n",
        "    - Seq2Seq 모델은 입력 시퀀스와 해당 시퀀스에 대응하는 출력 시퀀스로 구성된 데이터셋을 필요로 합니다. 예를 들어, 영어 문장과 그에 대응하는 한국어 번역 문장 쌍이 학습 데이터가 될 수 있습니다.\n",
        "2. **인코딩 단계**\n",
        "    - 입력 시퀀스는 인코더를 통해 처리되고, 이로부터 **컨텍스트 벡터(context vector)** 또는 **인코딩 벡터**가 생성됩니다. 이 벡터는 입력 시퀀스의 전체 의미를 압축한 것입니다.\n",
        "3. **디코딩 단계**\n",
        "    - 디코더는 인코더로부터 전달된 벡터를 기반으로 출력 시퀀스를 생성합니다. 처음에는 시작 토큰 `<SOS>`로 시작하고, 이후 단어를 하나씩 생성하며 출력 시퀀스를 만들어 나갑니다.\n",
        "    >> Seq2Seq\n",
        "    - 각 시점에서의 출력은 다음 시점의 입력으로 사용되며, 마지막에는 종료 토큰 `<EOS>`가 생성되면 출력이 종료됩니다.\n",
        "\n",
        "### Seq2Seq의 문제점과 해결책\n",
        "\n",
        "1. **고정된 벡터 표현의 한계**\n",
        "    - 초기의 Seq2Seq 모델은 인코더가 입력 시퀀스 전체를 **고정된 크기의 벡터**로 압축한 후 디코더로 전달합니다. 긴 시퀀스의 경우 정보를 잃을 위험이 있어 **성능 저하**가 발생할 수 있습니다.\n",
        "2. **어텐션 메커니즘(Attention Mechanism)**\n",
        "    - *어텐션(Attention)**은 Seq2Seq 모델의 성능을 대폭 개선한 방법입니다.\n",
        "    - 어텐션 메커니즘은 디코더가 출력 시퀀스를 생성할 때, 인코더의 모든 숨겨진 상태를 참조하도록 하여 특정 시점에서 입력의 어떤 부분에 더 집중해야 하는지 판단하게 합니다. 이를 통해 긴 문장을 처리할 때 정보 손실을 줄일 수 있습니다.\n",
        "    - 어텐션을 사용하는 Transformer 모델이 기존의 Seq2Seq RNN 기반 모델보다 더 좋은 성능을 보이며, 특히 번역 작업에서 널리 사용됩니다.\n",
        "\n",
        "### Transformer 기반 Seq2Seq 모델\n",
        "\n",
        "**Transformer**는 RNN 구조를 사용하지 않고 완전히 **어텐션 메커니즘**을 기반으로 하는 Seq2Seq 모델입니다. Transformer는 인코더와 디코더로 구성되며, 각 부분에서 자기 어텐션(Self-Attention)과 **포지션-와이즈 피드포워드 네트워크**를 사용합니다.\n",
        "\n",
        "- **인코더**는 입력 시퀀스를 여러 레이어의 자기 어텐션과 피드포워드 네트워크를 통해 변환하여 입력의 중요한 특징을 추출합니다.\n",
        "- **디코더**는 인코더에서 받은 인코딩 결과와 이전에 생성한 출력 단어들을 참조하여 새로운 단어를 생성합니다.\n",
        "- Transformer의 가장 큰 장점은 모든 단어에 대해 병렬 처리가 가능하다는 것입니다. 이는 모델의 **학습 속도**와 **성능**을 크게 향상시킵니다.\n",
        "\n",
        "### Seq2Seq 모델의 응용\n",
        "\n",
        "1. **기계 번역(Machine Translation)**\n",
        "    - 입력 문장을 다른 언어로 번역하는 작업에서 Seq2Seq 모델이 사용됩니다. 예를 들어, 영어 문장을 한국어로 번역하는 경우, 인코더는 영어 문장을 이해하고, 디코더는 이 정보를 바탕으로 한국어 문장을 생성합니다.\n",
        "2. **텍스트 요약(Text Summarization)**\n",
        "    - 긴 문서를 요약하는 작업에서도 Seq2Seq 모델이 사용됩니다. 인코더는 긴 입력 문서의 중요한 정보를 압축하고, 디코더는 이를 바탕으로 간결한 요약을 생성합니다.\n",
        "3. **질문 답변(Question Answering)**\n",
        "    - 질문을 입력으로 받아 해당하는 답변을 생성하는 작업에서도 Seq2Seq 모델을 사용할 수 있습니다.\n",
        "4. **대화 생성(Dialogue Generation)**\n",
        "    - 사용자의 입력을 받아 적절한 응답을 생성하는 챗봇과 같은 시스템에도 Seq2Seq 모델이 사용됩니다.\n",
        "    \n",
        "\n",
        "---\n",
        "\n",
        "- **Seq2Seq 모델**은 하나의 시퀀스를 다른 시퀀스로 변환하는 데 특화된 모델로, 인코더와 디코더라는 두 가지 주요 구성 요소로 이루어져 있습니다.\n",
        "- **Transformer**는 Seq2Seq의 한 종류로, 어텐션 메커니즘을 사용하여 더 나은 성능을 발휘합니다.\n",
        "- Seq2Seq 모델은 자연어 처리에서 번역, 요약, 대화 생성 등 다양한 작업에 사용되며, 특히 어텐션 메커니즘의 도입으로 긴 시퀀스를 처리하는 능력이 크게 향상되었습니다."
      ],
      "metadata": {
        "id": "is10kJksOmUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfM5re7pOfRc"
      },
      "outputs": [],
      "source": [
        "# Transformer seq2seq 를 이용한 영어 한국어 번역\n",
        "# Seq2Seq 모델은 성능이 상당히 안좋은 편. 그 중 그나마 나은게 트랜스포머라 한번 시도만 해본다.\n",
        "\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "# M2M100 모델 및 토크나이저 불러오기 / Load M2M100 model and tokenizer\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 영어 문장을 한국어로 번역하는 함수 / Function to Translate English to Korean\n",
        "def translate_english_to_korean(text):\n",
        "    tokenizer.src_lang = \"en\"  # 원본 언어를 영어로 설정 / Set source language to English\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # 번역된 토큰 생성 / Generate translated tokens\n",
        "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
        "\n",
        "    # 번역된 텍스트 디코딩 / Decode translated text\n",
        "    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "# 번역 실행 / Execute Translation\n",
        "if __name__ == \"__main__\":\n",
        "    english_text = \"The greatest glory in living lies not in never falling, but in rising every time we fall. .\"\n",
        "\n",
        "    # 번역 결과 출력 / Output Translation\n",
        "    translated_text = translate_english_to_korean(english_text)\n",
        "    print(\"Input English Sentence:\", english_text)\n",
        "    print(\"Translated Korean Sentence:\", translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 요약 예시\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "# KoBART 모델 및 토크나이저 불러오기 / Load KoBART model and tokenizer\n",
        "model_name = \"gogamza/kobart-summarization\"\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 한국어 텍스트를 요약하는 함수 / Function to Summarize Korean Text\n",
        "def summarize_korean_text(text):\n",
        "    # 입력 텍스트 토크나이징 / Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # 요약 생성 / Generate summary  - RAG에서 많이 쓰일 것.\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=100,               # 요약된 텍스트의 최대 길이를 줄임 : 문서를 구분할 때 상당히 중요.\n",
        "        num_beams=4,                  # 빔 서치\n",
        "        repetition_penalty=3.0,       # 반복 방지 벌점 설정.\n",
        "        length_penalty=1.5,           # 길이 패널티 설정\n",
        "        no_repeat_ngram_size=3,       # 3-그램 반복 방지 설정\n",
        "        early_stopping=True,\n",
        "        temperature=0.7               # 온도 설정을 낮게 조절\n",
        "    )\n",
        "\n",
        "    # 요약된 텍스트 디코딩 / Decode summarized text\n",
        "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary_text\n",
        "\n",
        "# 요약 실행 / Execute Summary\n",
        "if __name__ == \"__main__\":\n",
        "    korean_text = (\n",
        "        \"오늘 날씨가 정말 좋습니다. 저는 공원에서 산책을 하고 싶습니다. \"\n",
        "        \"어제는 비가 많이 와서 공원에 가지 못했지만, 오늘은 맑은 하늘과 따뜻한 온도 덕분에 기분이 좋습니다.\"\n",
        "    )\n",
        "\n",
        "    # 요약 결과 출력 / Output Summary\n",
        "    summarized_text = summarize_korean_text(korean_text)\n",
        "    print(\"Input Korean Text:\", korean_text)\n",
        "    print(\"Summarized Korean Text:\", summarized_text)\n"
      ],
      "metadata": {
        "id": "VnIcQkZGOmEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBart 모델을 이용한 치킨 주문시스템 챗봇 구현\n",
        "# KoBart는 연습용.\n",
        "# 챗봇은 의도를 파악하는 것이 중요함.\n",
        "# 규칙 기반은 정확할 수 있지만 학습 외에는 답변을 받을 수 없음 // 학습 기반은 정확도는 떨어지지만 유연함\n",
        "\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "import random\n",
        "\n",
        "# KoBART 모델 및 토크나이저 불러오기 / Load KoBART model and tokenizer\n",
        "model_name = \"gogamza/kobart-summarization\"\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 치킨집 주문 처리 챗봇 / Chicken Shop Order Processing Chatbot\n",
        "def chicken_order_chatbot(user_input):\n",
        "    # 사용자가 요청한 주문 정보 처리 / Process user's order request\n",
        "    if \"가격\" in user_input or \"비용\" in user_input or \"얼마\" in user_input:\n",
        "        return check_price(user_input)\n",
        "    elif any(word in user_input for word in [\"메뉴\", \"추천\", \"치킨 종류\"]):\n",
        "        return recommend_menu()\n",
        "    elif any(word in user_input for word in [\"주문\", \"시키다\", \"배달\"]):\n",
        "        return process_order(user_input)\n",
        "    elif any(word in user_input for word in [\"배달\", \"배송\", \"배달 가능\"]):\n",
        "        return \"네, 배달이 가능합니다. 주문하실 치킨 종류를 말씀해 주세요.\"\n",
        "    else:\n",
        "        # KoBART 모델을 사용하여 응답 생성 / Generate response using KoBART for other questions\n",
        "        return generate_response_with_kobart(user_input)\n",
        "\n",
        "# 메뉴 추천 기능 / Recommend Menu\n",
        "def recommend_menu():\n",
        "    menu = [\"후라이드 치킨\", \"양념 치킨\", \"반반 치킨\", \"간장 치킨\", \"허니 갈릭 치킨\"]\n",
        "    return f\"저희 추천 메뉴는 {random.choice(menu)}입니다. 어떤 치킨을 주문하시겠어요?\"\n",
        "\n",
        "# 가격 확인 기능 / Check Price\n",
        "def check_price(order_text):\n",
        "    # 가격 정보 딕셔너리 / Price information dictionary\n",
        "    prices = {\n",
        "        \"후라이드\": 15000,\n",
        "        \"양념\": 16000,\n",
        "        \"반반\": 16000,\n",
        "        \"간장\": 17000,\n",
        "        \"허니 갈릭\": 18000\n",
        "    }\n",
        "\n",
        "    for chicken, price in prices.items():\n",
        "        if chicken in order_text:\n",
        "            # 주문 개수 파악 / Detect the number of orders\n",
        "            count = 1  # 기본 1마리로 가정 / Default to 1 chicken\n",
        "            words = order_text.split()\n",
        "            for word in words:\n",
        "                if \"마리\" in word:\n",
        "                    try:\n",
        "                        count = int(word.replace(\"마리\", \"\"))\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "            total_price = price * count\n",
        "            return f\"{chicken} 치킨 {count}마리의 가격은 {total_price:,}원입니다.\"\n",
        "\n",
        "    return \"주문 가능한 치킨의 종류는 후라이드, 양념, 반반, 간장, 허니 갈릭입니다.\"\n",
        "\n",
        "# 주문 처리 기능 / Process Order\n",
        "def process_order(order_text):\n",
        "    chicken_types = [\"후라이드\", \"양념\", \"반반\", \"간장\", \"허니 갈릭\"]\n",
        "    for chicken in chicken_types:\n",
        "        if chicken in order_text:\n",
        "            return f\"{chicken} 치킨을 주문하셨습니다. 배달 주소와 전화번호를 알려주세요.\"\n",
        "    return \"주문하실 치킨 종류를 다시 말씀해 주세요.\"\n",
        "\n",
        "# KoBART를 사용하여 응답 생성 / Generate response using KoBART\n",
        "def generate_response_with_kobart(user_input):\n",
        "    # 입력 텍스트 토크나이징 / Tokenize input text\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # 응답 생성 / Generate response\n",
        "    response_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=100,\n",
        "        num_beams=4,\n",
        "        repetition_penalty=3.0,   # 반복 방지 벌점 설정 / Set repetition penalty\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=3,   # 3-그램 반복 방지 설정 / Set n-gram repeat prevention\n",
        "        early_stopping=True,\n",
        "        temperature=0.7           # 온도 설정 / Set temperature for diversity in generation\n",
        "    )\n",
        "\n",
        "    # 생성된 응답 디코딩 / Decode generated response\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# 챗봇 실행 / Run Chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"안녕하세요! 치킨 주문을 도와드리겠습니다. 질문이 있으시면 말씀해주세요.\")\n",
        "    while True:\n",
        "        user_input = input(\"사용자: \")\n",
        "        if user_input.lower() in [\"종료\", \"끝\", \"그만\"]:\n",
        "            print(\"챗봇: 이용해 주셔서 감사합니다. 좋은 하루 되세요!\")\n",
        "            break\n",
        "\n",
        "        response = chicken_order_chatbot(user_input)\n",
        "        print(\"챗봇:\", response)"
      ],
      "metadata": {
        "id": "bbvK5PXdO6Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음은 KoBart 모델을 이용한 기차표 주문시스템 챗봇의 구현입니다.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "# KoBART 모델 및 토크나이저 불러오기 / Load KoBART model and tokenizer\n",
        "model_name = \"gogamza/kobart-summarization\"\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 다양한 기차 및 거리별 요금 데이터 / Train types and fare data\n",
        "train_data = {\n",
        "    \"출발지\": [\"서울\", \"서울\", \"서울\", \"대전\", \"대전\", \"부산\", \"부산\", \"광주\"],\n",
        "    \"도착지\": [\"대전\", \"부산\", \"광주\", \"부산\", \"광주\", \"서울\", \"대전\", \"서울\"],\n",
        "    \"거리(km)\": [150, 400, 300, 250, 180, 400, 250, 300],\n",
        "    \"KTX 요금(원)\": [20000, 50000, 35000, 30000, 25000, 50000, 30000, 35000],\n",
        "    \"무궁화호 요금(원)\": [10000, 25000, 20000, 18000, 15000, 25000, 18000, 20000],\n",
        "    \"새마을호 요금(원)\": [15000, 35000, 30000, 25000, 20000, 35000, 25000, 30000]\n",
        "}\n",
        "train_df = pd.DataFrame(train_data)\n",
        "\n",
        "# 기차 요금 조회 기능 / Function to Get Train Fare\n",
        "def get_train_fare(user_input):\n",
        "    # 사용자의 입력에서 출발지, 도착지, 기차 종류를 추출 / Extract departure, arrival, and train type from user input\n",
        "    pattern = r\"(\\w+)에서 (\\w+)까지 (\\w+) 요금\"\n",
        "    match = re.search(pattern, user_input)\n",
        "\n",
        "    if match:\n",
        "        departure = match.group(1)\n",
        "        arrival = match.group(2)\n",
        "        train_type = match.group(3).upper()  # 기차 종류를 대문자로 변환 / Convert train type to uppercase\n",
        "\n",
        "        # 기차 종류에 맞는 열 이름 찾기 / Find the correct fare column based on train type\n",
        "        fare_column = \"\"\n",
        "        if train_type == \"KTX\":\n",
        "            fare_column = \"KTX 요금(원)\"\n",
        "        elif train_type == \"무궁화호\":\n",
        "            fare_column = \"무궁화호 요금(원)\"\n",
        "        elif train_type == \"새마을호\":\n",
        "            fare_column = \"새마을호 요금(원)\"\n",
        "        else:\n",
        "            return \"KTX, 무궁화호, 새마을호 중 하나를 선택해 주세요.\"\n",
        "\n",
        "        # 데이터프레임에서 출발지, 도착지에 해당하는 요금을 찾기 / Find the fare in the dataframe\n",
        "        result = train_df[(train_df['출발지'] == departure) & (train_df['도착지'] == arrival)]\n",
        "\n",
        "        if not result.empty:\n",
        "            distance = result.iloc[0]['거리(km)']\n",
        "            fare = result.iloc[0][fare_column]\n",
        "            return f\"{departure}에서 {arrival}까지의 거리는 {distance}km이며, {train_type} 요금은 {fare:,}원입니다.\"\n",
        "        else:\n",
        "            return \"해당 경로에 대한 요금 정보를 찾을 수 없습니다. 출발지와 도착지를 다시 확인해 주세요.\"\n",
        "    else:\n",
        "        return generate_response_with_kobart(user_input)\n",
        "\n",
        "# KoBART를 사용하여 응답 생성 / Generate response using KoBART\n",
        "def generate_response_with_kobart(user_input):\n",
        "    # 입력 텍스트 토크나이징 / Tokenize input text\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # 응답 생성 / Generate response\n",
        "    response_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=100,\n",
        "        num_beams=4,\n",
        "        repetition_penalty=3.0,   # 반복 방지 벌점 설정 / Set repetition penalty\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=3,   # 3-그램 반복 방지 설정 / Set n-gram repeat prevention\n",
        "        early_stopping=True,\n",
        "        temperature=0.7           # 온도 설정 / Set temperature for diversity in generation\n",
        "    )\n",
        "\n",
        "    # 생성된 응답 디코딩 / Decode generated response\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# 챗봇 실행 / Run Chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"안녕하세요! 기차 요금을 안내해 드리겠습니다. 출발지와 도착지, 기차 종류를 말씀해 주세요. 예: '서울에서 부산까지 KTX 요금'\")\n",
        "    while True:\n",
        "        user_input = input(\"사용자: \")\n",
        "        if user_input.lower() in [\"종료\", \"끝\", \"그만\"]:\n",
        "            print(\"챗봇: 이용해 주셔서 감사합니다. 좋은 하루 되세요!\")\n",
        "            break\n",
        "\n",
        "        response = get_train_fare(user_input)\n",
        "        print(\"챗봇:\", response)"
      ],
      "metadata": {
        "id": "D5ZG8ZAVPFAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# M2M100을 사용해 영어 문장을 한국어로 번역한 후, gpt-3.5-turbo를 사용해 번역의 자연스러움을 개선.\n",
        "\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "import openai\n",
        "import os\n",
        "\n",
        "\n",
        "openai.api_key = ''\n",
        "\n",
        "# M2M100 모델 및 토크나이저 불러오기 / Load M2M100 model and tokenizer\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 영어를 한국어로 번역하는 함수 / Function to translate English to Korean\n",
        "def translate_english_to_korean(text):\n",
        "    tokenizer.src_lang = \"en\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
        "    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "# LLM을 사용하여 번역의 자연스러움 개선 / Improve Translation with LLM\n",
        "def improve_translation_with_llm(text):\n",
        "    # LLM을 사용하여 자연스러운 번역 생성 / Generate a more natural translation using LLM\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that helps improve translations to make them more natural and fluent.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please improve the following Korean translation to make it more natural and fluent:\\n\\n{text}\\n\\nImproved Translation:\"}\n",
        "    ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=1.0,\n",
        "        n=1\n",
        "    )\n",
        "\n",
        "    improved_text = response.choices[0].message['content'].strip()\n",
        "    return improved_text\n",
        "\n",
        "# 번역 및 개선 실행 / Execute Translation and Improvement\n",
        "if __name__ == \"__main__\":\n",
        "    # 입력 영어 문장 / Input English Sentence\n",
        "    english_text = \"The weather today is really nice. I'm thinking of going for a walk in the park.\"\n",
        "\n",
        "    # 1. M2M100을 사용한 초기 번역 / Initial translation using M2M100\n",
        "    initial_translation = translate_english_to_korean(english_text)\n",
        "    print(\"Input English Sentence:\", english_text)\n",
        "    print(\"Initial Translation (Korean):\", initial_translation)\n",
        "\n",
        "    # 2. LLM을 사용하여 번역의 자연스러움 개선 / Improve translation using LLM\n",
        "    improved_translation = improve_translation_with_llm(initial_translation)\n",
        "    print(\"Improved Translation (Korean):\", improved_translation)"
      ],
      "metadata": {
        "id": "iPBb7YbcPLrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI GPT3.5를 이용한 펜션 답변 시스템\n",
        "\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = ''  # 여기에 본인의 API 키를 입력하세요\n",
        "\n",
        "# 특정 펜션 정보 데이터\n",
        "pension_info = {\n",
        "    \"오션뷰 펜션\": {\n",
        "        \"위치\": \"강릉\",\n",
        "        \"가격(1박/원)\": 150000,\n",
        "        \"예약 가능 여부\": \"가능\",\n",
        "        \"BBQ 가능 여부\": \"가능\",\n",
        "        \"식사 제공 여부\": \"제공 안 함\",\n",
        "        \"전화번호\": \"010-1234-5678\",\n",
        "        \"애완동물 동반 여부\": \"불가능\",\n",
        "        \"방 갯수\": 3,\n",
        "        \"편의시설\": [\"Wi-Fi\", \"주차\", \"TV\", \"냉장고\", \"에어컨\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 펜션 정보 질문 처리 함수\n",
        "def get_pension_response(pension_name, question):\n",
        "    if pension_name in pension_info:\n",
        "        pension_details = pension_info[pension_name]\n",
        "\n",
        "        # 질문에 따라 적절한 정보를 제공\n",
        "        prompt = f\"{pension_name}에 대한 정보입니다: {pension_details}. 사용자 질문: {question}.\\n답변:\"\n",
        "\n",
        "        # GPT-3.5 Turbo API 호출\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    else:\n",
        "        return \"해당 펜션 정보를 찾을 수 없습니다.\"\n",
        "\n",
        "# 챗봇 실행\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"안녕하세요! 펜션에 대한 문의를 도와드리겠습니다.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"사용자: \")\n",
        "        if user_input.lower() in [\"종료\", \"끝\", \"그만\"]:\n",
        "            print(\"챗봇: 이용해 주셔서 감사합니다. 좋은 하루 되세요!\")\n",
        "            break\n",
        "\n",
        "        response = get_pension_response(pension_name, user_input)\n",
        "        print(\"챗봇:\", response)"
      ],
      "metadata": {
        "id": "QIjxHYYPPQLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}